# Radius Shrinkage for Adaptive\\Nearest Neighbors Classification

The k-nearest neighbor (k-NN) algorithm is a widely used non-parametric method for classification. However, its performance is highly sensitive to the choice of the number of neighbors, $k$, which directly impacts its ability to manage class imbalance, resist noise, smooth decision boundaries, and balance the bias-variance trade-off. In this work, we propose a novel adaptive k-NN algorithm that dynamically adjusts the neighborhood size by employing a radius shrinkage strategy based on the local geometric properties of the data manifold. Specifically, we leverage differential geometry principles to estimate local curvature, allowing points with low curvature to have larger radii and points with high curvature to have smaller radii. By approximating the local shape operator through the local Hessian and covariance matrices, we compute the local Gaussian curvature, which informs the adaptive radius adjustment. The proposed method demonstrates superior performance compared to traditional k-NN and other adaptive k-NN techniques, achieving higher balanced accuracy and Kappa coefficient on several real-world datasets. These results suggest that integrating differential geometry into neighborhood selection offers significant potential for improving non-parametric classifiers in complex data environments.
